{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw07_Sabine.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPna28qLG7t9fy9VymUuo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juelha/IANNWTF/blob/sabine/hw07_Sabine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.2 Generate a Tensorflow Datset\n",
        "You can of course now go on and compute a big bunch of examples and store\n",
        "them in a huge array or dataframe or whatever you like. However, that would\n",
        "be rather inefficient - as we work with random noise input, we can compute the\n",
        "data on the fly.\n",
        "Thus, we would like you to work with generators 1 and the tensorflow\n",
        "function tf.data.Dataset.from generator().\n",
        "There are a few simple steps to take:\n",
        "\n",
        "• Write a integration task(seq len, num sapmles) generator function: This function should for num samples times 2 yield a random noise\n",
        "signal of the size seq len 3\n",
        "(sequence length, thus the number of time steps\n",
        "2\n",
        "we are considering) and a target, namely if the sum of the noise signal is\n",
        "greater or smaller than 1. 4\n",
        "You make your life easier if you do not try to handle empty dimensions!5\n",
        "\n",
        "• Write a wrapper generator my integration task(): As it is easier to\n",
        "pass a generator to the tensorflow tf.data.Dataset.from generator()\n",
        "method that does not take any arguments, write a wrapper function\n",
        "which internally iterates to integration task with a specified seq len and\n",
        "num samples and yields the function’s yield (yes sounds stupid and complicated, but it’s really easier that way).\n",
        "Choose num samples to be rather high 6\n",
        ", as you do not want to overfit and\n",
        "you may want to start with a small sequence length for easier debugging.\n",
        "In the end, you should however be able to deal with a sequence length of\n",
        "25 at least.\n",
        "\n",
        "• Create the Tensorflow Dataset: Pass on your wrapper generator to\n",
        "tf.data.Dataset.from generator(, output signature= tf.TensorSpec(shape,\n",
        "dtype).\n",
        "\n"
      ],
      "metadata": {
        "id": "uuC2yWfmZyEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.integrate as integrate\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "BCcoJlXFapmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "ZSLHsA2wVBqN",
        "outputId": "2cbb5196-e985-4d16-c778-6c1ad9317bab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-4435c404bf49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mwrapper_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 instructions)\n\u001b[0;32m--> 552\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[0;34m(generator, output_types, output_shapes, args, output_signature, name)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         raise TypeError(\"To specify the output signature you need to provide \"\n\u001b[0m\u001b[1;32m    924\u001b[0m                         \u001b[0;34m\"either the `output_signature` argument or the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m                         \"`output_types` argument.\")\n",
            "\u001b[0;31mTypeError\u001b[0m: To specify the output signature you need to provide either the `output_signature` argument or the `output_types` argument."
          ]
        }
      ],
      "source": [
        "# Dataset Setup\n",
        "\n",
        "''''\n",
        "def integration_task(seq_len, num_samples): \n",
        "\n",
        "  for sample in range(num_samples):\n",
        "    f = lambda x: np.random.normal(loc=0, scale=2)\n",
        "    i = integrate.quad(f,0,seq_len)\n",
        "    print(i[0])\n",
        "\n",
        "integration_task(5, 1)\n",
        "\n",
        "'''\n",
        "\n",
        "def integration_task(seq_len, num_samples): \n",
        "  for sample in range(num_samples):\n",
        "    noise_signal=np.array([])\n",
        "    for signal in range(seq_len):\n",
        "      noise_signal = np.append(noise_signal, np.random.normal(loc=0, scale=2)) # noise signal ints of length seq_len\n",
        "    target = int(np.sum(noise_signal)>0)\n",
        "    noise_signal = np.expand_dims(noise_signal,-1) \n",
        "    #print(noise_signal.shape, target)\n",
        "    yield noise_signal, target\n",
        "\n",
        "def wrapper_generator(): \n",
        "  return integration_task(3,100)\n",
        "\n",
        "wrapper_generator()\n",
        "\n",
        "tf.data.Dataset.from_generator(wrapper_generator, output_signature=tf.TensorSpec(shape=(), dtype)"
      ]
    }
  ]
}